\label{alg:cuda_texture}
The algorithm is identical to the one described in \ref{alg:cuda_only}, except that the \verb|cuda_graph| is accessed through the texture memory of the device instead of the global memory. The term "texture memory" doesn't refer to a separate physical memory but rather it is an alternative way of accessing the global memory on the device \cite{web:nvidia_doc_texture_mem}: textures are stored on the global memory, and they are accessed through a dedicated read-only cache. The cache includes hardware filtering which can perform linear floating point interpolation as part of the read process. The cache, however, is different to a conventional cache, in that it is optimised for spatial locality (in the coordinate system of the texture) and not locality in memory. For some applications, this is ideal and will give a performance advantage, but for others, it won't: textures can be slower because access involves a cache miss penalty in addition to the global memory read.

The algorithm described in this chapter exploits the texture memory of CUDA by defining the arrays \verb|adj_lists| and \verb|adj_list_indexes| as textures mapped on the global memory. The idea is that the algorithm could benefit from the memory caching provided by the texture memory mechanism. However, textures have historically come in 2 usage models: \textbf{texture references} and \textbf{texture objects}. Texture references were the original mechanism supplied with CUDA and texture objects were introduced with the Kepler generation of GPUs. During the High Performance Computing course, only texture references were covered, but their usage was deprecated in CUDA 11.3. In order to exploit texture memory, the refactoring process described in \cite{web:noauthor_cuda_2013} was applied to the code seen in class.

Note that we didn't implement versions of the preprocessing algorithm exploiting shared or constant memories because they are too small for our purposes. In particular, observe that for each node of the graph, each CUDA thread must traverse the entirety of \verb|adj_lists| to look for edges incoming into that node. This makes it difficult to split the work among thread blocks, and thus it would be difficult to exploit the advantages of shared memory.